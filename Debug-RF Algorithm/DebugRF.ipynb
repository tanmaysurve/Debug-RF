{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08c1d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools as it\n",
    "import dare\n",
    "import copy\n",
    "import math\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import urllib.request as urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d954273",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Parent class for dataset preprocessing'''\n",
    "class Dataset():\n",
    "    def __init__(self, root = None, rootTrain = None, rootTest = None, column_names = None):\n",
    "        self.root = root\n",
    "        self.rootTrain = rootTrain\n",
    "        self.rootTest = rootTest\n",
    "        if self.root == None and (self.rootTrain == None or self.rootTest == None):\n",
    "            raise ValueError(\"Dataset root path not provided\")\n",
    "        self.column_names = column_names\n",
    "        self.dataset = pd.DataFrame()\n",
    "        self.trainDataset = pd.DataFrame()\n",
    "        self.testDataset = pd.DataFrame()\n",
    "        self.__loadDataset()\n",
    "        \n",
    "    def __loadDataset(self):\n",
    "        if not self.root == None: \n",
    "            if self.column_names == None:\n",
    "                self.dataset = pd.read_csv(self.root)\n",
    "            else:\n",
    "                self.dataset = pd.read_csv(self.root, \n",
    "                                           names = self.column_names, \n",
    "                                           sep = \",\")\n",
    "        if not (self.rootTrain == None or self.rootTest == None):\n",
    "            if self.column_names == None:\n",
    "                self.trainDataset = pd.read_csv(self.rootTrain)\n",
    "                self.testDataset = pd.read_csv(self.rootTest)\n",
    "            else:\n",
    "                self.trainDataset = pd.read_csv(self.rootTrain, \n",
    "                                           names = self.column_names, \n",
    "                                           sep = \",\")\n",
    "                self.testDataset = pd.read_csv(self.rootTest, \n",
    "                                           names = self.column_names, \n",
    "                                           sep = \",\")\n",
    "    \n",
    "    def getDataset(self):\n",
    "        pass\n",
    "    \n",
    "    def getDatasetWithNormalPreprocessing(self):\n",
    "        pass\n",
    "    \n",
    "    def getDatasetWithCategorizationPreprocessing(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf50717",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Enum class for different fairness metric that are supported in the code'''\n",
    "class FairnessMetric(Enum):\n",
    "        SP = 1\n",
    "        PP = 2\n",
    "        EO = 3\n",
    "        \n",
    "'''Main class implementing Debug-RF Algorithm'''\n",
    "class FairnessDebuggingUsingMachineUnlearning():\n",
    "    '''\n",
    "    dataloader = provided preprocessed dataset to work on (must be an instance of Dataset class)\n",
    "    sensitiveAttribute = [String], [sensitive attribute name in dataset, priviledgedClassValue, protectedClassValue]\n",
    "    classLabel = class label attribute name\n",
    "    fairnessMetric = Fairness metric to be used for the algorithm\n",
    "    '''\n",
    "    def __init__(self, dataloader, sensitiveAttribute, classLabel, fairnessMetric):\n",
    "        self.dataloader = dataloader\n",
    "        if self.dataloader == None:\n",
    "            raise ValueError(\"Dataset root path not provided\")\n",
    "        if not isinstance(self.dataloader, Dataset):\n",
    "            raise ValueError(\"Inappropriate class for dataloader object provided\")\n",
    "        sensitiveAttributeDictKeys = [\"Name\", \"Priviledged\", \"Protected\"]\n",
    "        self.sensitiveAttribute = {sensitiveAttributeDictKeys[i]: sensitiveAttribute[i] for i in range(0, len(sensitiveAttribute))}\n",
    "        self.classLabel = classLabel\n",
    "        if not isinstance(fairnessMetric, FairnessMetric):\n",
    "            raise ValueError(\"Inappropriate Enum class for fairnessMetric\")\n",
    "        self.fairnessMetric = fairnessMetric\n",
    "        self.train = pd.DataFrame()\n",
    "        self.test = pd.DataFrame()\n",
    "        self.trainX = pd.DataFrame()\n",
    "        self.trainY = pd.DataFrame()\n",
    "        self.testX = pd.DataFrame()\n",
    "        self.testY = pd.DataFrame()\n",
    "        self.testSensitiveAttr = pd.DataFrame()\n",
    "        self.trainSensitiveAttr = pd.DataFrame()\n",
    "        self.privTestIndices = None\n",
    "        self.protTestIndices = None\n",
    "        self.categorizedTrain = pd.DataFrame()\n",
    "        self.categorizedTest = pd.DataFrame()\n",
    "        self.columns = None\n",
    "        self.predictions = None\n",
    "        self.groundTruth = pd.DataFrame()\n",
    "        self.dataStatisticalParity = None\n",
    "        self.dataPredictiveParity = None\n",
    "        self.equalizingOddsParity = None\n",
    "        self.dataAccuracy = None\n",
    "        self.attributeMap = {}\n",
    "        self.literals = None\n",
    "        self.min = 1\n",
    "        self.max = 1000\n",
    "        self.validSubsetIndexLists = []\n",
    "        self.featureImportances = {}\n",
    "        self.sorted_indices_for_feature_importances = None;\n",
    "        self.__loadDataset()\n",
    "        self.__getPredictions()\n",
    "        self.__getFeatureImportances()\n",
    "        self.__createAttributeMap()\n",
    "        \n",
    "    def getDatasetFairnessParity(self):\n",
    "        if self.fairnessMetric == FairnessMetric.SP:\n",
    "            return self.getDatasetStatisticalParity()\n",
    "        elif self.fairnessMetric == FairnessMetric.PP:\n",
    "            return self.getDatasetPredictiveParity()\n",
    "        elif self.fairnessMetric == FairnessMetric.EO:\n",
    "            return self.getDatasetEqualizingOddsParity()\n",
    "    \n",
    "    def getDatasetStatisticalParity(self):\n",
    "        return self.dataStatisticalParity\n",
    "    \n",
    "    def getDatasetPredictiveParity(self):\n",
    "        return self.dataPredictiveParity\n",
    "        \n",
    "    def getDatasetEqualizingOddsParity(self):\n",
    "        return self.equalizingOddsParity\n",
    "    \n",
    "    def getAccuracy(self):\n",
    "        return str(self.dataAccuracy * 100) + \"%\"\n",
    "    \n",
    "    def getLiterals(self):\n",
    "        return self.literals\n",
    "    \n",
    "    def getAttributeMap(self):\n",
    "        return self.attributeMap\n",
    "        \n",
    "    def __loadDataset(self):\n",
    "        self.train, self.test = self.dataloader.getDatasetWithNormalPreprocessing()\n",
    "        self.trainX = self.train.drop(self.classLabel, axis = 1)\n",
    "        self.trainY = self.train[self.classLabel]\n",
    "        self.testX = self.test.drop(self.classLabel, axis = 1)\n",
    "        self.testY = self.test[self.classLabel]\n",
    "        self.testSensitiveAttr = self.testX[self.sensitiveAttribute[\"Name\"]]\n",
    "        self.trainSensitiveAttr = self.trainX[self.sensitiveAttribute[\"Name\"]]\n",
    "        self.privTestIndices = np.where(self.testSensitiveAttr == self.sensitiveAttribute[\"Priviledged\"])[0]\n",
    "        self.protTestIndices = np.where(self.testSensitiveAttr == self.sensitiveAttribute[\"Protected\"])[0]\n",
    "        self.categorizedTrain, self.categorizedTest = self.dataloader.getDatasetWithCategorizationPreprocessing(decodeAttributeValues = True)\n",
    "        self.columns = self.categorizedTrain.columns.values\n",
    "        '''Feature scaling to standardize dataset to help model learn patterns'''\n",
    "        for col in self.trainX.columns:     \n",
    "            scaler = StandardScaler()     \n",
    "            self.trainX[col] = scaler.fit_transform(self.trainX[col].values.reshape(-1, 1))\n",
    "        for col in self.testX.columns:     \n",
    "            scaler = StandardScaler()     \n",
    "            self.testX[col] = scaler.fit_transform(self.testX[col].values.reshape(-1, 1)) \n",
    "    \n",
    "    def __getPredictions(self):\n",
    "        '''\n",
    "        n_estimators = no. of trees\n",
    "        max_depth = depth of trees\n",
    "        k = no. thresholds to consider per attribute\n",
    "        topd = no. of random node layers\n",
    "        '''\n",
    "        rf = dare.Forest(n_estimators = 100,\n",
    "                         max_depth = 5,\n",
    "                         k = 8,  \n",
    "                         topd = 1,  \n",
    "                         random_state = 1)\n",
    "        rf.fit(self.trainX.to_numpy(), self.trainY.to_numpy())\n",
    "        self.predictions = rf.predict(self.testX.to_numpy())\n",
    "        self.groundTruth = self.testY\n",
    "        self.dataAccuracy = accuracy_score(self.testY.to_numpy(), self.predictions)\n",
    "        self.dataStatisticalParity = self.__getStatisticalParityDifference(self.privTestIndices, self.protTestIndices, self.predictions)\n",
    "        self.dataPredictiveParity = self.__getPredictiveParityDifference(self.privTestIndices, self.protTestIndices, self.predictions)\n",
    "        self.equalizingOddsParity = self.__getEqualizingOddsParityDifference(self.privTestIndices, self.protTestIndices, self.predictions)\n",
    "    \n",
    "    def __getFeatureImportances(self):\n",
    "        rf = RandomForestClassifier(n_estimators = 100,\n",
    "                                    max_depth = 5)\n",
    "        rf.fit(self.trainX.to_numpy(), self.trainY.to_numpy())\n",
    "        importances = rf.feature_importances_\n",
    "        self.sorted_indices_for_feature_importances = np.argsort(importances)[::-1]\n",
    "        for i, column in enumerate(self.trainX.columns):\n",
    "            self.featureImportances[column] = importances[i];\n",
    "        \n",
    "    def __getFairnessParityDifference(self, priviledgedIndices, protectedIndices, predictions):\n",
    "        if self.fairnessMetric == FairnessMetric.SP:\n",
    "            return self.__getStatisticalParityDifference(priviledgedIndices, protectedIndices, predictions)\n",
    "        elif self.fairnessMetric == FairnessMetric.PP:\n",
    "            return self.__getPredictiveParityDifference(priviledgedIndices, protectedIndices, predictions)\n",
    "        elif self.fairnessMetric == FairnessMetric.EO:\n",
    "            return self.__getEqualizingOddsParityDifference(priviledgedIndices, protectedIndices, predictions) \n",
    "    \n",
    "    def __getStatisticalParityDifference(self, priviledgedIndices, protectedIndices, predictions):\n",
    "        y_pred_priviledged = predictions[priviledgedIndices]\n",
    "        y_pred_protected = predictions[protectedIndices]\n",
    "        spPri = len(np.where(y_pred_priviledged == 1)[0]) / (len(y_pred_priviledged) + 1)\n",
    "        spPro = len(np.where(y_pred_protected == 1)[0]) / (len(y_pred_protected) + 1)\n",
    "        return spPri - spPro\n",
    "    \n",
    "    def __getPredictiveParityDifference(self, priviledgedIndices, protectedIndices, predictions):\n",
    "        y_pred_priviledged = predictions[priviledgedIndices]\n",
    "        y_pred_protected = predictions[protectedIndices]\n",
    "        ppPri = precision_score(self.groundTruth[priviledgedIndices].to_numpy(), y_pred_priviledged)\n",
    "        ppPro = precision_score(self.groundTruth[protectedIndices].to_numpy(), y_pred_protected)\n",
    "        return ppPri - ppPro\n",
    "        \n",
    "    def __getEqualizingOddsParityDifference(self, priviledgedIndices, protectedIndices, predictions):\n",
    "        y_pred_priviledged = predictions[priviledgedIndices]\n",
    "        y_pred_protected = predictions[protectedIndices]\n",
    "        \n",
    "        cnf_matrix_pri = confusion_matrix(self.groundTruth[priviledgedIndices].to_numpy(), y_pred_priviledged)\n",
    "        FP_pri = (cnf_matrix_pri.sum(axis=0) - np.diag(cnf_matrix_pri)).astype(float)  \n",
    "        FN_pri = (cnf_matrix_pri.sum(axis=1) - np.diag(cnf_matrix_pri)).astype(float)\n",
    "        TP_pri = (np.diag(cnf_matrix_pri)).astype(float)\n",
    "        TN_pri = (cnf_matrix_pri.sum() - (FP_pri + FN_pri + TP_pri)).astype(float)\n",
    "        # Sensitivity, hit rate, recall, or true positive rate\n",
    "        TPR_pri = TP_pri / (TP_pri + FN_pri)\n",
    "        # Fall out or false positive rate\n",
    "        FPR_pri = FP_pri/(FP_pri + TN_pri)\n",
    "        \n",
    "        cnf_matrix_pro = confusion_matrix(self.groundTruth[protectedIndices].to_numpy(), y_pred_protected)\n",
    "        FP_pro = (cnf_matrix_pro.sum(axis=0) - np.diag(cnf_matrix_pro)).astype(float)  \n",
    "        FN_pro = (cnf_matrix_pro.sum(axis=1) - np.diag(cnf_matrix_pro)).astype(float)\n",
    "        TP_pro = (np.diag(cnf_matrix_pro)).astype(float)\n",
    "        TN_pro = (cnf_matrix_pro.sum() - (FP_pro + FN_pro + TP_pro)).astype(float)\n",
    "        # Sensitivity, hit rate, recall, or true positive rate\n",
    "        TPR_pro = TP_pro / (TP_pro + FN_pro)\n",
    "        # Fall out or false positive rate\n",
    "        FPR_pro = FP_pro/(FP_pro + TN_pro)\n",
    "        \n",
    "        TPR_parity = TPR_pri[1] - TPR_pro[1]\n",
    "        FPR_parity = FPR_pri[1] - FPR_pro[1]\n",
    "        eo_parity = (TPR_parity + FPR_parity) / 2\n",
    "        return eo_parity\n",
    "    \n",
    "    def __createAttributeMap(self):\n",
    "        categorizedDf = pd.concat([self.categorizedTrain, self.categorizedTest], ignore_index=True)\n",
    "        for column in categorizedDf.columns:\n",
    "            if column != self.classLabel:\n",
    "                for value in categorizedDf[column].unique():\n",
    "                    self.attributeMap[value] = column\n",
    "\n",
    "        self.literals = np.concatenate([categorizedDf[col].unique() for col in categorizedDf if col != self.classLabel])\n",
    "        \n",
    "    def __doesSubsetAlreadyExist(self, subset, existingSubsets):\n",
    "        for item in existingSubsets:\n",
    "            if(item.difference(subset) == set()):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def __isSubsetRealistic(self, subset):\n",
    "        columnDict = dict.fromkeys(self.columns, 0)\n",
    "        for item in subset:\n",
    "            if(columnDict[self.attributeMap[item]] > 0):\n",
    "                return False\n",
    "            else:\n",
    "                columnDict[self.attributeMap[item]] += 1\n",
    "        return True\n",
    "    \n",
    "    def __getSubsetInfo(self, subset, minSupport, maxSupport):\n",
    "        requiredIndices = pd.Series(dtype = 'int') \n",
    "        firstItem = True\n",
    "        for item in subset:\n",
    "            if(firstItem == True):\n",
    "                requiredIndices = (self.categorizedTrain[self.attributeMap[item]] == item)\n",
    "                firstItem = False\n",
    "            requiredIndices = requiredIndices & (self.categorizedTrain[self.attributeMap[item]] == item)\n",
    "        subsetIndexLists = (self.categorizedTrain[requiredIndices].index.tolist())\n",
    "        support =  len(subsetIndexLists) / len(self.categorizedTrain)\n",
    "        isValidSubset = (support >= minSupport) and (support <= maxSupport)\n",
    "        return {\"isValid\": isValidSubset, \"indexList\": subsetIndexLists, \"support\": support}\n",
    "    \n",
    "    def __evaluateSubset(self, subsetIndexList, p1Parity, p1Size, p2Parity, p2Size, compare, shouldCompareOrigParity):\n",
    "        start = time.time()\n",
    "        rf_temp = dare.Forest(n_estimators = 100,\n",
    "                              max_depth = 5, \n",
    "                              k = 8,  \n",
    "                              topd = 1,  \n",
    "                              random_state = 1)\n",
    "        rf_temp.fit(self.trainX.to_numpy(), self.trainY.to_numpy())\n",
    "        end = time.time()\n",
    "        timeElapsedToTrain = end - start\n",
    "        start = time.time()\n",
    "        rf_temp.delete(subsetIndexList)\n",
    "        end = time.time()\n",
    "        timeElapsedToDelete = end - start\n",
    "        predictions = rf_temp.predict(self.testX.to_numpy())\n",
    "        accuracy = accuracy_score(self.testY.to_numpy(), predictions)\n",
    "        parityChildValue = self.__getFairnessParityDifference(self.privTestIndices, self.protTestIndices, predictions)\n",
    "        isBetterThanParents = False\n",
    "        if compare == 'normal':\n",
    "            if shouldCompareOrigParity:\n",
    "                isBetterThanParents = abs(parityChildValue) < abs(p1Parity) and abs(parityChildValue) < abs(p2Parity) and abs(parityChildValue) < abs(self.getDatasetFairnessParity())\n",
    "            else:\n",
    "                isBetterThanParents = abs(parityChildValue) < abs(p1Parity) and abs(parityChildValue) < abs(p2Parity)\n",
    "        elif compare == \"per_instance\":\n",
    "            avgChildValue = parityChildValue / len(subsetIndexList)\n",
    "            avgP1Value = p1Parity / p1Size\n",
    "            avgP2Value = p2Parity / p2Size\n",
    "            avgDatasetParity = self.getDatasetFairnessParity() / len(self.trainX)\n",
    "            if shouldCompareOrigParity:\n",
    "                isBetterThanParents = abs(avgChildValue) < abs(avgP1Value) and abs(avgChildValue) < abs(avgP2Value) and abs(avgChildValue) < abs(avgDatasetParity)\n",
    "            else:\n",
    "                isBetterThanParents = abs(avgChildValue) < abs(avgP1Value) and abs(avgChildValue) < abs(avgP2Value)\n",
    "        return {\"isBetterThanParents\": isBetterThanParents, \"parity\": parityChildValue, \"accuracy\": accuracy,\n",
    "                \"timeElapsedToTrain\": timeElapsedToTrain, \"timeElapsedToDelete\": timeElapsedToDelete}\n",
    "        \n",
    "    def __expandSubsets(self, N, L):\n",
    "        E = []\n",
    "        if L == 0:\n",
    "            for column in self.literals:\n",
    "                E.append({\"subset\": {str(column)}, \"parity\": 0, \"size\": self.min,\n",
    "                          \"parent1\": {\"parity\": self.max, \"size\": self.min},\n",
    "                          \"parent2\": {\"parity\": self.max, \"size\": self.min}\n",
    "                         })\n",
    "            return E\n",
    "        newSubsets = []\n",
    "        for index1, parent1 in enumerate(N):\n",
    "            for index2, parent2 in enumerate(N):\n",
    "                if(index2 <= index1):\n",
    "                    continue\n",
    "                setIntersection = parent1[\"subset\"].intersection(parent2[\"subset\"])\n",
    "                if(len(setIntersection) != (L - 1)):\n",
    "                    continue \n",
    "                setUnion = parent1[\"subset\"].union(parent2[\"subset\"])\n",
    "                if(self.__doesSubsetAlreadyExist(setUnion, newSubsets) == True):\n",
    "                    continue\n",
    "                if(self.__isSubsetRealistic(setUnion) == False):\n",
    "                    continue    \n",
    "                newSubsets.append(setUnion)\n",
    "                E.append({\"subset\": setUnion, \"parity\": 0, \"size\": self.min,\n",
    "                          \"parent1\": {\"parity\": parent1[\"parity\"], \"size\": parent1[\"size\"]},\n",
    "                          \"parent2\": {\"parity\": parent2[\"parity\"], \"size\": parent2[\"size\"]}\n",
    "                         })\n",
    "        return E\n",
    "    \n",
    "    '''\n",
    "    maxLiterals - Int, max number of literals which can be in subsets\n",
    "    subsetSupportRange - Int or [Int, Int] or (Int, Int), support range in which subset should lie, [min, max],\n",
    "                        If single value is provided range is [0.05, max].\n",
    "    compare - {\"normal\", \"per_instance\"}\n",
    "    shouldCompareOrigParity = True / False\n",
    "    isPruning - True / False\n",
    "    '''\n",
    "    def latticeSearchSubsets(self, maxLiterals = 2, subsetSupportRange = 0.1, compare = \"normal\", shouldCompareOrigParity = True, isPruning = True):\n",
    "        if isinstance(subsetSupportRange, (list, tuple, np.ndarray)):\n",
    "            if subsetSupportRange[0] > subsetSupportRange[1]:\n",
    "                raise ValueError(\"Min value cannot be greater than Max\")\n",
    "            minSupport, maxSupport = subsetSupportRange[0], subsetSupportRange[1]\n",
    "        else:\n",
    "            minSupport, maxSupport = 0.01, subsetSupportRange\n",
    "        if maxSupport == 0.0:\n",
    "            maxSupport += 0.1\n",
    "        L = 0\n",
    "        L_notPruning = 0\n",
    "        E = self.__expandSubsets([], 0)\n",
    "        N = []\n",
    "        validSubsetsInfo = []\n",
    "        self.validSubsetIndexLists = []\n",
    "        while L <= (maxLiterals - 1):\n",
    "            if isPruning:\n",
    "                print(\"level: \" + str(L))\n",
    "            else:\n",
    "                print(\"level: \" + str(L_notPruning))\n",
    "            for subset in E:\n",
    "                subsetInfo = self.__getSubsetInfo(subset[\"subset\"], minSupport, maxSupport)\n",
    "                subset[\"size\"] = len(subsetInfo[\"indexList\"])\n",
    "                if subsetInfo[\"isValid\"] == False:\n",
    "                    if subsetInfo[\"support\"] > maxSupport:\n",
    "                        N.append(subset)\n",
    "                    else:\n",
    "                        continue\n",
    "                subsetEvalResult = self.__evaluateSubset(subsetInfo[\"indexList\"],\n",
    "                                                         subset[\"parent1\"][\"parity\"], subset[\"parent1\"][\"size\"],\n",
    "                                                         subset[\"parent2\"][\"parity\"], subset[\"parent2\"][\"size\"],\n",
    "                                                         compare, shouldCompareOrigParity)\n",
    "                subset[\"parity\"] = subsetEvalResult[\"parity\"]\n",
    "                isSubsetValid = False\n",
    "                if isPruning:\n",
    "                    isSubsetValid = subsetEvalResult[\"isBetterThanParents\"] == True and subsetInfo[\"support\"] <= maxSupport\n",
    "                else:\n",
    "                    isSubsetValid = subsetInfo[\"support\"] <= maxSupport\n",
    "                if isSubsetValid:\n",
    "                    self.validSubsetIndexLists.append(subsetInfo[\"indexList\"])\n",
    "                    N.append(subset)\n",
    "                    validSubsetsInfo.append({\"subset\": subset[\"subset\"],\n",
    "                                             \"size\": len(subsetInfo[\"indexList\"]),\n",
    "                                             \"support\": subsetInfo[\"support\"],\n",
    "                                             \"parity\": subset[\"parity\"],\n",
    "                                             \"accuracy\": subsetEvalResult[\"accuracy\"],\n",
    "                                             \"timeElapsedToTrain\": subsetEvalResult[\"timeElapsedToTrain\"],\n",
    "                                             \"timeElapsedToDelete\": subsetEvalResult[\"timeElapsedToDelete\"]})\n",
    "            if isPruning:\n",
    "                L += 1\n",
    "                E = self.__expandSubsets(N, L)\n",
    "            else:\n",
    "                L_notPruning += 1\n",
    "                E = self.__expandSubsets(N, L_notPruning)\n",
    "            if not E:\n",
    "                break\n",
    "        return self.__getResults(validSubsetsInfo)\n",
    "        \n",
    "    def __getGroundTruthValues(self):\n",
    "        gt_parity = []\n",
    "        gt_accuracy = []\n",
    "        for indexList in self.validSubsetIndexLists:\n",
    "            rf_temp = dare.Forest(n_estimators = 100,\n",
    "                          max_depth = 5,\n",
    "                          k = 8,  \n",
    "                          topd = 1,  \n",
    "                          random_state = 1)\n",
    "            newX = self.trainX.drop(self.trainX.index[indexList]) \n",
    "            newY = self.trainY.drop(self.trainY.index[indexList])\n",
    "            rf_temp.fit(newX.to_numpy(), newY.to_numpy())\n",
    "            predictions = rf_temp.predict(self.testX.to_numpy())\n",
    "            accuracy = accuracy_score(self.testY.to_numpy(), predictions)\n",
    "            parity = self.__getFairnessParityDifference(self.privTestIndices, self.protTestIndices, predictions)\n",
    "            gt_accuracy.append(accuracy)\n",
    "            gt_parity.append(parity)\n",
    "        return gt_parity, gt_accuracy\n",
    "    \n",
    "    def __getResults(self, validSubsetsInfo):\n",
    "        gt_parity, gt_accuracy = self.__getGroundTruthValues()\n",
    "        result = pd.DataFrame(columns = [\"Subset\", \"Size\", \"Support\", \"Parity\", \"GT_Parity\", \n",
    "                                         \"Accuracy\", \"GT_Accuracy\", \"timeElapsedToTrain\", \n",
    "                                         \"timeElapsedToDelete\", \"Parity_Reduction\", \"Accuracy_Reduction\"])\n",
    "        for index, subset in enumerate(validSubsetsInfo):\n",
    "            parityReduction = (abs(self.getDatasetFairnessParity()) - abs(subset[\"parity\"])) * 100 / abs(self.getDatasetFairnessParity()) \n",
    "            accReduction = (abs(self.dataAccuracy) - abs(subset[\"accuracy\"])) * 100 / abs(self.dataAccuracy)\n",
    "            newResultEntry = [str(subset[\"subset\"]),\n",
    "                              str(subset[\"size\"]),\n",
    "                              str(subset[\"support\"]),\n",
    "                              str(subset[\"parity\"]),\n",
    "                              str(gt_parity[index]),\n",
    "                              str(subset[\"accuracy\"]),\n",
    "                              str(gt_accuracy[index]),\n",
    "                              str(subset[\"timeElapsedToTrain\"]),\n",
    "                              str(subset[\"timeElapsedToDelete\"]),\n",
    "                              str(parityReduction),\n",
    "                              str(accReduction)]\n",
    "            result.loc[len(result)] = newResultEntry\n",
    "        self.getEstimatedVsGroundTruthParityGraph(result)\n",
    "        self.getEstimatedVsGroundTruthAccuracyGraph(result)\n",
    "        result[\"Parity_Reduction\"] = result[\"Parity_Reduction\"].astype(str).astype(float)\n",
    "        result = result.sort_values('Parity_Reduction', ascending = False, ignore_index = True)\n",
    "        return result\n",
    "    \n",
    "    def getEstimatedVsGroundTruthParityGraph(self, result):\n",
    "        if len(result) == 0:\n",
    "            return\n",
    "        x = np.linspace(-10, 10, 1000)\n",
    "        estParity = result['Parity'].astype(float).to_numpy()\n",
    "        gtParity = result['GT_Parity'].astype(float).to_numpy()\n",
    "        minEst, maxEst = estParity.min(), estParity.max()\n",
    "        minGt, maxGt = gtParity.min(), gtParity.max()\n",
    "        minValue = minEst if minEst < minGt else minGt\n",
    "        maxValue = maxEst if maxEst > maxGt else maxGt\n",
    "        plt.figure(figsize = (10,10))\n",
    "        plt.title(\"Estimated Parity vs Ground Truth Parity\")\n",
    "        plt.xlabel(\"Estimated Parity\")\n",
    "        plt.ylabel(\"Ground Truth Parity\")\n",
    "        plt.xlim(minValue, maxValue)\n",
    "        plt.ylim(minValue, maxValue)\n",
    "        plt.plot(estParity, gtParity, 'ro')\n",
    "        plt.plot(x, x + 0, '-g')\n",
    "        plt.show()\n",
    "        \n",
    "    def getEstimatedVsGroundTruthAccuracyGraph(self, result):\n",
    "        if len(result) == 0:\n",
    "            return\n",
    "        x = np.linspace(-10, 10, 1000)\n",
    "        estAcc = result['Accuracy'].astype(float).to_numpy()\n",
    "        gtAcc = result['GT_Accuracy'].astype(float).to_numpy()\n",
    "        minEst, maxEst = estAcc.min(), estAcc.max()\n",
    "        minGt, maxGt = gtAcc.min(), gtAcc.max()\n",
    "        minValue = minEst if minEst < minGt else minGt\n",
    "        maxValue = maxEst if maxEst > maxGt else maxGt\n",
    "        plt.figure(figsize = (10,10))\n",
    "        plt.title(\"Estimated Accuracy vs Ground Truth Accuracy\")\n",
    "        plt.xlabel(\"Estimated Accuracy\")\n",
    "        plt.ylabel(\"Ground Truth Accuracy\")\n",
    "        plt.xlim(minValue, maxValue)\n",
    "        plt.ylim(minValue, maxValue)\n",
    "        plt.plot(estAcc, gtAcc, 'ro')\n",
    "        plt.plot(x, x + 0, '-g')\n",
    "        plt.show()\n",
    "        \n",
    "    def drawInferencesFromResultSubsets(self, subsets, protectedName, priviledgedName):\n",
    "        result = pd.DataFrame(columns = [\"Subset\", \"Size\", \"Support\", \"SupportRange\", \n",
    "                                         \"Total_\" + protectedName, \"Total_\" + priviledgedName,\n",
    "                                         protectedName + \"_1s\", priviledgedName + \"_1s\", \n",
    "                                         protectedName + \"_0s\", priviledgedName + \"_0s\"])\n",
    "        for i in range(0,len(subsets) + 1):\n",
    "            subset = None\n",
    "            subsetIndexLists = None\n",
    "            supportRange = None\n",
    "            if i != 0:\n",
    "                subset = subsets[i-1]\n",
    "                requiredIndices = pd.Series(dtype = 'int') \n",
    "                firstItem = True\n",
    "                for item in subset:\n",
    "                    if(firstItem == True):\n",
    "                        requiredIndices = (self.categorizedTrain[self.attributeMap[item]] == item)\n",
    "                        firstItem = False\n",
    "                    requiredIndices = requiredIndices & (self.categorizedTrain[self.attributeMap[item]] == item)\n",
    "                subsetIndexLists = (self.categorizedTrain[requiredIndices].index.tolist())\n",
    "            else:\n",
    "                subset = \"Entire Train Dataset\"\n",
    "                subsetIndexLists = (self.categorizedTrain.index.tolist())\n",
    "            subsetSensitiveGroupData = self.trainSensitiveAttr[subsetIndexLists]\n",
    "            privIndices = np.where(subsetSensitiveGroupData == self.sensitiveAttribute[\"Priviledged\"])[0]\n",
    "            protIndices = np.where(subsetSensitiveGroupData == self.sensitiveAttribute[\"Protected\"])[0]\n",
    "            support =  len(subsetIndexLists) / len(self.categorizedTrain)\n",
    "            if support >= 0 and support < 0.05: \n",
    "                supportRange = \"LT5%\"\n",
    "            elif support >= 0.05 and support < 0.10:\n",
    "                supportRange = \"5-10%\"\n",
    "            elif support >= 0.10 and support < 0.30: \n",
    "                supportRange = \"10-30%\"\n",
    "            elif support >= 0.30 and support < 1:\n",
    "                supportRange = \"GT30%\"\n",
    "            else: \n",
    "                supportRange = \"100%\"\n",
    "            gt_status = self.trainY.to_numpy()\n",
    "            eps = 0.00001\n",
    "            protected_1s = len(np.where(gt_status[protIndices] == 1)[0])\n",
    "            priviledged_1s = len(np.where(gt_status[privIndices] == 1)[0])\n",
    "            protected_0s = len(np.where(gt_status[protIndices] == 0)[0])\n",
    "            priviledged_0s = len(np.where(gt_status[privIndices] == 0)[0])\n",
    "            newResultEntry = [str(subset),\n",
    "                              str(len(subsetIndexLists)),\n",
    "                              str(support),\n",
    "                              str(supportRange),\n",
    "                              str(len(protIndices)),\n",
    "                              str(len(privIndices)),\n",
    "                              str(round(protected_1s / (len(protIndices) + eps), 2)),\n",
    "                              str(round(priviledged_1s / (len(privIndices) + eps), 2)),\n",
    "                              str(round(protected_0s / (len(protIndices) + eps), 2)),\n",
    "                              str(round(priviledged_0s / (len(privIndices) + eps), 2))]\n",
    "            result.loc[len(result)] = newResultEntry\n",
    "        return result\n",
    "    \n",
    "    def getFeatureImportanceChanges(self, subsets):\n",
    "        cols = [\"Subset\"]\n",
    "        datasetCols = self.trainX.columns\n",
    "        for index in self.sorted_indices_for_feature_importances:\n",
    "            cols.append(datasetCols[index])\n",
    "        result = pd.DataFrame(columns = cols)\n",
    "        for subset in subsets:\n",
    "            requiredIndices = pd.Series(dtype = 'int') \n",
    "            firstItem = True\n",
    "            for item in subset:\n",
    "                if(firstItem == True):\n",
    "                    requiredIndices = (self.categorizedTrain[self.attributeMap[item]] == item)\n",
    "                    firstItem = False\n",
    "                requiredIndices = requiredIndices & (self.categorizedTrain[self.attributeMap[item]] == item)\n",
    "            subsetIndexLists = (self.categorizedTrain[requiredIndices].index.tolist())\n",
    "            rf = RandomForestClassifier(n_estimators = 100,\n",
    "                                        max_depth = 5)\n",
    "            newX = self.trainX.drop(self.trainX.index[subsetIndexLists]) \n",
    "            newY = self.trainY.drop(self.trainY.index[subsetIndexLists])\n",
    "            rf.fit(newX.to_numpy(), newY.to_numpy())\n",
    "            importances = rf.feature_importances_\n",
    "            newResultEntry = [str(subset)]\n",
    "            for i in self.sorted_indices_for_feature_importances:\n",
    "                featureImpChange = ((importances[i] - self.featureImportances[datasetCols[i]]) * 100) / self.featureImportances[datasetCols[i]]\n",
    "                newResultEntry.append(str(featureImpChange))\n",
    "            result.loc[len(result)] = newResultEntry\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d936a745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
